{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f08c6a2e-8f02-4338-ab0c-702d258459fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the data\n",
    "import pandas as pd\n",
    "from typing import List, Callable\n",
    "from multiprocess.pool import Pool\n",
    "import string\n",
    "from tqdm import tqdm\n",
    "\n",
    "# List of tokens that donot hold much importance.\n",
    "# These stopwords are part of NLTK library but we \n",
    "# are hardcoding them to avoid unnecessary dependency on\n",
    "# NLTK library\n",
    "MAX_WORKERS = 20\n",
    "STOP_WORDS = {'call', 'upon', 'still', 'nevertheless', 'down', 'every', 'forty', '‘re', 'always', 'whole', 'side', \"n't\", 'now', 'however', 'an', 'show', 'least', 'give', 'below', 'did', 'sometimes', 'which', \"'s\", 'nowhere', 'per', 'hereupon', 'yours', 'she', 'moreover', 'eight', 'somewhere', 'within', 'whereby', 'few', 'has', 'so', 'have', 'for', 'noone', 'top', 'were', 'those', 'thence', 'eleven', 'after', 'no', '’ll', 'others', 'ourselves', 'themselves', 'though', 'that', 'nor', 'just', '’s', 'before', 'had', 'toward', 'another', 'should', 'herself', 'and', 'these', 'such', 'elsewhere', 'further', 'next', 'indeed', 'bottom', 'anyone', 'his', 'each', 'then', 'both', 'became', 'third', 'whom', '‘ve', 'mine', 'take', 'many', 'anywhere', 'to', 'well', 'thereafter', 'besides', 'almost', 'front', 'fifteen', 'towards', 'none', 'be', 'herein', 'two', 'using', 'whatever', 'please', 'perhaps', 'full', 'ca', 'we', 'latterly', 'here', 'therefore', 'us', 'how', 'was', 'made', 'the', 'or', 'may', '’re', 'namely', \"'ve\", 'anyway', 'amongst', 'used', 'ever', 'of', 'there', 'than', 'why', 'really', 'whither', 'in', 'only', 'wherein', 'last', 'under', 'own', 'therein', 'go', 'seems', '‘m', 'wherever', 'either', 'someone', 'up', 'doing', 'on', 'rather', 'ours', 'again', 'same', 'over', '‘s', 'latter', 'during', 'done', \"'re\", 'put', \"'m\", 'much', 'neither', 'among', 'seemed', 'into', 'once', 'my', 'otherwise', 'part', 'everywhere', 'never', 'myself', 'must', 'will', 'am', 'can', 'else', 'although', 'as', 'beyond', 'are', 'too', 'becomes', 'does', 'a', 'everyone', 'but', 'some', 'regarding', '‘ll', 'against', 'throughout', 'yourselves', 'him', \"'d\", 'it', 'himself', 'whether', 'move', '’m', 'hereafter', 're', 'while', 'whoever', 'your', 'first', 'amount', 'twelve', 'serious', 'other', 'any', 'off', 'seeming', 'four', 'itself', 'nothing', 'beforehand', 'make', 'out', 'very', 'already', 'various', 'until', 'hers', 'they', 'not', 'them', 'where', 'would', 'since', 'everything', 'at', 'together', 'yet', 'more', 'six', 'back', 'with', 'thereupon', 'becoming', 'around', 'due', 'keep', 'somehow', 'n‘t', 'across', 'all', 'when', 'i', 'empty', 'nine', 'five', 'get', 'see', 'been', 'name', 'between', 'hence', 'ten', 'several', 'from', 'whereupon', 'through', 'hereby', \"'ll\", 'alone', 'something', 'formerly', 'without', 'above', 'onto', 'except', 'enough', 'become', 'behind', '’d', 'its', 'most', 'n’t', 'might', 'whereas', 'anything', 'if', 'her', 'via', 'fifty', 'is', 'thereby', 'twenty', 'often', 'whereafter', 'their', 'also', 'anyhow', 'cannot', 'our', 'could', 'because', 'who', 'beside', 'by', 'whence', 'being', 'meanwhile', 'this', 'afterwards', 'whenever', 'mostly', 'what', 'one', 'nobody', 'seem', 'less', 'do', '‘d', 'say', 'thus', 'unless', 'along', 'yourself', 'former', 'thru', 'he', 'hundred', 'three', 'sixty', 'me', 'sometime', 'whose', 'you', 'quite', '’ve', 'about', 'even'}\n",
    "\n",
    "# Dataset downloaded from Kaggle.\n",
    "SENTIMENTS = '/Users/hope/Downloads/test.csv'\n",
    "df = pd.read_csv(SENTIMENTS, encoding='ISO-8859-1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8f4a79d7-98e0-40f6-8801-b30975058391",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Utility functions\n",
    "\n",
    "def split_df(df: pd.DataFrame, chunk_size: int = 2000) -> List[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Function to split a pandas DF/series into chunks\n",
    "    \"\"\"\n",
    "    index = 0\n",
    "    splits = []\n",
    "    while index < len(df.index):\n",
    "        splits.append(df.iloc[index:index+chunk_size])\n",
    "        index += chunk_size\n",
    "    return splits\n",
    "\n",
    "\n",
    "\n",
    "def parallel_apply(col: pd.Series, func: Callable) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Utility function to apply a function to all the entries\n",
    "    of a series. \n",
    "\n",
    "    Internal series.apply works sequentially. This function replicates\n",
    "    the behavior but runs parallaly on all the rows.\n",
    "    \"\"\"\n",
    "    \n",
    "    splits = split_df(col, 100)\n",
    "    with Pool(MAX_WORKERS) as pool:\n",
    "        results = []\n",
    "        for split in splits:\n",
    "            task = pool.apply_async(lambda x: x.apply(func), (split, ))\n",
    "            results.append(task)\n",
    "        parsed_splits = [future.get() for future in tqdm(results)]\n",
    "    return pd.concat(parsed_splits)\n",
    "\n",
    "\n",
    "def clean_text(doc: str) -> List[str]:\n",
    "    \"\"\"\n",
    "    Returns a list of clean tokens for a given line.\n",
    "    \"\"\"\n",
    "    \n",
    "    # split into tokens by white space\n",
    "    tokens = doc.split()\n",
    "    # remove punctuation from each token\n",
    "    table = str.maketrans('', '', string.punctuation)\n",
    "    tokens = [w.translate(table) for w in tokens]\n",
    "    # remove remaining tokens that are not alphabetic\n",
    "    tokens = [word for word in tokens if word.isalpha()]\n",
    "    # filter out stop words\n",
    "    stop_words = STOP_WORDS\n",
    "    tokens = [w for w in tokens if not w in stop_words]\n",
    "    # filter out short tokens\n",
    "    tokens = [word.lower() for word in tokens if len(word) > 1]\n",
    "    return tokens\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4e34190-5bbc-4d86-ac8f-966e56883a8e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████| 36/36 [00:00<00:00, 274536.26it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[last, session, day]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[shanghai, exciting, precisely, skyscrapers, g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[recession, hit, veronique, branquinho, quit, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[happy, bday]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[like]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3529</th>\n",
       "      <td>its at 3 am, im very tired but i can`t sleep  ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[im, tired, cant, sleep, try]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3530</th>\n",
       "      <td>All alone in this old house again.  Thanks for...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[all, old, house, thanks, net, keeps, alive, k...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3531</th>\n",
       "      <td>I know what you mean. My little dog is sinkin...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[know, mean, my, little, dog, sinking, depress...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3532</th>\n",
       "      <td>_sutra what is your next youtube video gonna b...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[sutra, youtube, video, gonna, love, videos]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3533</th>\n",
       "      <td>http://twitpic.com/4woj2 - omgssh  ang cute n...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[omgssh, ang, cute, ng, bby]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>3534 rows × 3 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                   text sentiment  \\\n",
       "0     Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1      Shanghai is also really exciting (precisely -...  positive   \n",
       "2     Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3                                           happy bday!  positive   \n",
       "4                http://twitpic.com/4w75p - I like it!!  positive   \n",
       "...                                                 ...       ...   \n",
       "3529  its at 3 am, im very tired but i can`t sleep  ...  negative   \n",
       "3530  All alone in this old house again.  Thanks for...  positive   \n",
       "3531   I know what you mean. My little dog is sinkin...  negative   \n",
       "3532  _sutra what is your next youtube video gonna b...  positive   \n",
       "3533   http://twitpic.com/4woj2 - omgssh  ang cute n...  positive   \n",
       "\n",
       "                                           cleaned_text  \n",
       "0                                  [last, session, day]  \n",
       "1     [shanghai, exciting, precisely, skyscrapers, g...  \n",
       "2     [recession, hit, veronique, branquinho, quit, ...  \n",
       "3                                         [happy, bday]  \n",
       "4                                                [like]  \n",
       "...                                                 ...  \n",
       "3529                      [im, tired, cant, sleep, try]  \n",
       "3530  [all, old, house, thanks, net, keeps, alive, k...  \n",
       "3531  [know, mean, my, little, dog, sinking, depress...  \n",
       "3532       [sutra, youtube, video, gonna, love, videos]  \n",
       "3533                       [omgssh, ang, cute, ng, bby]  \n",
       "\n",
       "[3534 rows x 3 columns]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cleaning the data\n",
    "# Cleaning without multiprocessing as this is not a bottle neck\n",
    "\n",
    "# 1. We only care about the text and corresponding sentiments\n",
    "df = df[[\"text\", \"sentiment\"]]\n",
    "\n",
    "# 2. Dropping the bad rows\n",
    "df = df[~df.sentiment.isna()]\n",
    "\n",
    "\n",
    "# Cleaning with multiprocessing for bottlenecks\n",
    "# 3. Creating a cleaned text column parallely. This improves the runtime\n",
    "df[\"cleaned_text\"] = parallel_apply(df.text, clean_text)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a0b2419f-5495-4b6e-a363-d79c8c0c5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████| 36/36 [00:00<00:00, 452.39it/s]\n",
      "100%|███████████████████████████████████████| 36/36 [00:00<00:00, 653657.77it/s]\n"
     ]
    }
   ],
   "source": [
    "# Feature engineering\n",
    "# Will be done parallely\n",
    "# Heavy task\n",
    "# - Create a vector matrix for each token and for each text.\n",
    "\n",
    "# Step 1: Extracting all distinct tokens. Fast and doesnot require\n",
    "# parallisation. Infact parallisation will reduce the overall performance\n",
    "ALL_TOKENS = df.cleaned_text.sum()\n",
    "\n",
    "def get_top_tokens(token_list, num_tokens = 3000):\n",
    "    mp = {}\n",
    "    for token in token_list:\n",
    "        mp[token] = mp.get(token, 0) + 1\n",
    "    mp = sorted(mp.items(), key=lambda item: item[1], reverse=True)\n",
    "    top_tokens = []\n",
    "    for token, freq in mp:\n",
    "        top_tokens.append(token)\n",
    "    return top_tokens[:500]\n",
    "\n",
    "# Step 2: Creating feature matrix. Heavy task. As this will create a 2d space\n",
    "# of Number of sentence x number of total tokens. will do using parallisation\n",
    "ALL_TOKENS = get_top_tokens(ALL_TOKENS)\n",
    "def create_vector_array(tokens):\n",
    "    vector = []\n",
    "    for token in ALL_TOKENS:\n",
    "        if token in tokens:\n",
    "            vector.append(1)\n",
    "        else:\n",
    "            vector.append(0)\n",
    "    return vector\n",
    "\n",
    "df[\"features\"] = parallel_apply(df.cleaned_text, create_vector_array)\n",
    "df[\"values\"] = parallel_apply(df.sentiment, lambda value: 1 if value == \"positive\" else 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6ebb1bd3-bba1-4048-bc48-fcab4aeaf87d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 3. Dropping all the neutral sentiments as these will cause \n",
    "# issues while training a logistic regresssion. Logistic regressions\n",
    "# need binary classifications. positive/negative works good with it. \n",
    "# positive can be treated as 1 and negative as 0. \n",
    "# The predictions will be a real number between 0 - 1. The closer the\n",
    "# value to 1. The more positive the sentiment\n",
    "train_df = df[df.sentiment != \"neutral\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "57cbe0cd-9933-4d22-ac96-9bccc188cf50",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "features = train_df.features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "b524885f-fd81-41bd-8ed5-2d886c76d363",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = train_df[\"values\"].to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eaa7cb82-4a11-436d-b3c1-c88605cf3809",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "efbf8f52-0af9-4503-9ff8-6de9e7ac0273",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training the model\n",
    "# We have explored the concept of bagging where multiple models are generated parallely.\n",
    "# Result is Average of all the models. It's supposed to give a more generalised result.\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "clf = LogisticRegression\n",
    "\n",
    "class BaggedRegressions:\n",
    "    def __init__(self, num_estimators: int):\n",
    "        self.num_estimators = num_estimators\n",
    "\n",
    "    def train(self, features, values):\n",
    "        tol = 0.0001\n",
    "        delta_tol = 0.0001\n",
    "        with Pool(MAX_WORKERS) as pool:\n",
    "            tasks = []\n",
    "            for index in range(self.num_estimators):\n",
    "                tasks.append(pool.apply_async(lambda tol: LogisticRegression(tol=tol).fit(features.to_list(), values), (tol, )))\n",
    "                tol += delta_tol\n",
    "            self.models = []\n",
    "            current_task = 1\n",
    "            for task in tasks:\n",
    "                self.models.append(task.get())\n",
    "                print(f\"Got {current_task} / {self.num_estimators} models\")\n",
    "                current_task += 1\n",
    "\n",
    "    def predict_probablity(self, feature):\n",
    "        prob = 0\n",
    "        for model in self.models:\n",
    "            prob += model.predict_proba([feature])[0][1]\n",
    "        return prob / self.num_estimators"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3cbdb598-3987-4838-abb9-392e7bfe94d0",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Got 1 / 20 models\n",
      "Got 2 / 20 models\n",
      "Got 3 / 20 models\n",
      "Got 4 / 20 models\n",
      "Got 5 / 20 models\n",
      "Got 6 / 20 models\n",
      "Got 7 / 20 models\n",
      "Got 8 / 20 models\n",
      "Got 9 / 20 models\n",
      "Got 10 / 20 models\n",
      "Got 11 / 20 models\n",
      "Got 12 / 20 models\n",
      "Got 13 / 20 models\n",
      "Got 14 / 20 models\n",
      "Got 15 / 20 models\n",
      "Got 16 / 20 models\n",
      "Got 17 / 20 models\n",
      "Got 18 / 20 models\n",
      "Got 19 / 20 models\n",
      "Got 20 / 20 models\n"
     ]
    }
   ],
   "source": [
    "bagged_regressions = BaggedRegressions(20)\n",
    "bagged_regressions.train(features, values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4929201c-726d-4fea-add9-d4dddf721948",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5027905455183762"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(df.features.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "004fbde3-9b15-4a17-8abe-953a94d991af",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████| 36/36 [00:00<00:00, 41.20it/s]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "df[\"prediction_prob\"] = parallel_apply(df.features, lambda x: bagged_regressions.predict_probablity(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c2f6dee1-47cb-4d55-9eff-51ab05dfa6d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment</th>\n",
       "      <th>cleaned_text</th>\n",
       "      <th>features</th>\n",
       "      <th>values</th>\n",
       "      <th>prediction_prob</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Last session of the day  http://twitpic.com/67ezh</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[last, session, day]</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.502791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Shanghai is also really exciting (precisely -...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[shanghai, exciting, precisely, skyscrapers, g...</td>\n",
       "      <td>[0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.830085</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Recession hit Veronique Branquinho, she has to...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[recession, hit, veronique, branquinho, quit, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.296053</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>happy bday!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[happy, bday]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.831863</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>http://twitpic.com/4w75p - I like it!!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[like]</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.539352</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>that`s great!! weee!! visitors!</td>\n",
       "      <td>positive</td>\n",
       "      <td>[thats, great, weee, visitors]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.914360</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>I THINK EVERYONE HATES ME ON HERE   lol</td>\n",
       "      <td>negative</td>\n",
       "      <td>[think, everyone, hates, me, on, here, lol]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.223599</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>soooooo wish i could, but im in school and my...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[soooooo, wish, im, school, myspace, completel...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.572136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>and within a short time of the last clue all ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[short, time, clue]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.681904</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>What did you get?  My day is alright.. haven`...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[what, my, day, alright, havent, leaving, soon...</td>\n",
       "      <td>[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.331292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>My bike was put on hold...should have known th...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[my, bike, holdshould, known, argh, total, bum...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.319157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>I checked.  We didn`t win</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[checked, we, didnt, win]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.278437</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>.. and you`re on twitter! Did the tavern bore...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[youre, twitter, did, tavern, bore]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.517576</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>I`m in VA for the weekend, my youngest son tur...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[im, va, weekend, youngest, son, turns, tomorr...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.020588</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>Its coming out the socket  I feel like my phon...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[its, coming, socket, feel, like, phones, hole...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.199922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>So hot today =_=  don`t like it and i hate my ...</td>\n",
       "      <td>negative</td>\n",
       "      <td>[so, hot, today, dont, like, hate, new, timeta...</td>\n",
       "      <td>[0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.021933</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>Miss you</td>\n",
       "      <td>negative</td>\n",
       "      <td>[miss]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.077709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>Cramps . . .</td>\n",
       "      <td>negative</td>\n",
       "      <td>[cramps]</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.460108</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>you guys didn`t say hi or answer my questions...</td>\n",
       "      <td>positive</td>\n",
       "      <td>[guys, didnt, hi, answer, questions, yesterday...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>1</td>\n",
       "      <td>0.847923</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>I`m going into a spiritual stagnentation, its ...</td>\n",
       "      <td>neutral</td>\n",
       "      <td>[im, going, spiritual, stagnentation, explodin...</td>\n",
       "      <td>[1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>0</td>\n",
       "      <td>0.799466</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 text sentiment  \\\n",
       "0   Last session of the day  http://twitpic.com/67ezh   neutral   \n",
       "1    Shanghai is also really exciting (precisely -...  positive   \n",
       "2   Recession hit Veronique Branquinho, she has to...  negative   \n",
       "3                                         happy bday!  positive   \n",
       "4              http://twitpic.com/4w75p - I like it!!  positive   \n",
       "5                     that`s great!! weee!! visitors!  positive   \n",
       "6             I THINK EVERYONE HATES ME ON HERE   lol  negative   \n",
       "7    soooooo wish i could, but im in school and my...  negative   \n",
       "8    and within a short time of the last clue all ...   neutral   \n",
       "9    What did you get?  My day is alright.. haven`...   neutral   \n",
       "10  My bike was put on hold...should have known th...  negative   \n",
       "11                          I checked.  We didn`t win   neutral   \n",
       "12   .. and you`re on twitter! Did the tavern bore...   neutral   \n",
       "13  I`m in VA for the weekend, my youngest son tur...  negative   \n",
       "14  Its coming out the socket  I feel like my phon...  negative   \n",
       "15  So hot today =_=  don`t like it and i hate my ...  negative   \n",
       "16                                           Miss you  negative   \n",
       "17                                       Cramps . . .  negative   \n",
       "18   you guys didn`t say hi or answer my questions...  positive   \n",
       "19  I`m going into a spiritual stagnentation, its ...   neutral   \n",
       "\n",
       "                                         cleaned_text  \\\n",
       "0                                [last, session, day]   \n",
       "1   [shanghai, exciting, precisely, skyscrapers, g...   \n",
       "2   [recession, hit, veronique, branquinho, quit, ...   \n",
       "3                                       [happy, bday]   \n",
       "4                                              [like]   \n",
       "5                      [thats, great, weee, visitors]   \n",
       "6         [think, everyone, hates, me, on, here, lol]   \n",
       "7   [soooooo, wish, im, school, myspace, completel...   \n",
       "8                                 [short, time, clue]   \n",
       "9   [what, my, day, alright, havent, leaving, soon...   \n",
       "10  [my, bike, holdshould, known, argh, total, bum...   \n",
       "11                          [checked, we, didnt, win]   \n",
       "12                [youre, twitter, did, tavern, bore]   \n",
       "13  [im, va, weekend, youngest, son, turns, tomorr...   \n",
       "14  [its, coming, socket, feel, like, phones, hole...   \n",
       "15  [so, hot, today, dont, like, hate, new, timeta...   \n",
       "16                                             [miss]   \n",
       "17                                           [cramps]   \n",
       "18  [guys, didnt, hi, answer, questions, yesterday...   \n",
       "19  [im, going, spiritual, stagnentation, explodin...   \n",
       "\n",
       "                                             features  values  prediction_prob  \n",
       "0   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.502791  \n",
       "1   [0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1         0.830085  \n",
       "2   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.296053  \n",
       "3   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, ...       1         0.831863  \n",
       "4   [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1         0.539352  \n",
       "5   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1         0.914360  \n",
       "6   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, ...       0         0.223599  \n",
       "7   [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.572136  \n",
       "8   [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, ...       0         0.681904  \n",
       "9   [0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.331292  \n",
       "10  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.319157  \n",
       "11  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.278437  \n",
       "12  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.517576  \n",
       "13  [1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.020588  \n",
       "14  [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.199922  \n",
       "15  [0, 0, 0, 1, 0, 0, 1, 0, 0, 0, 0, 1, 0, 0, 0, ...       0         0.021933  \n",
       "16  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.077709  \n",
       "17  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       0         0.460108  \n",
       "18  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...       1         0.847923  \n",
       "19  [1, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, ...       0         0.799466  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "8bc960b8-45ff-406a-9532-16ddbe9a7d63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_to_feature(text: str):\n",
    "    text = clean_text(text)\n",
    "    return create_vector_array(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f2555998-2ffd-4b43-97c3-e7c777df9b58",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9504756933577841"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"I am a very good person. Today I am happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "deb4f46c-f53f-4c03-a7ac-fc4da7236cfc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0029226843225160703"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"I am a very sad person. I hate my life\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "42b2f276-6a72-4836-beda-92a9b49849f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6614674063108554"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"Today was a decent day\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a76abc60-d283-49f0-a235-e76ce0652142",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4371464121568714"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"Life is what it is\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6ba720e5-378c-4d5e-9dd0-3bcebe0589e2",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7910127779398521"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"College life is fun\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "9f680db5-04cf-44fc-86a0-3bbf31c323b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8354645736628861"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bagged_regressions.predict_probablity(text_to_feature(\"I saved a life today. It made me happy\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1cabd786-170d-4397-a4ec-bf7a784322d8",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
